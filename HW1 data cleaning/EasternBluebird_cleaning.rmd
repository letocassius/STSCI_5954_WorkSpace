---
title: "Data cleaning for Exploratory Data Analysis Eastern Bluebird Capstone"
author: "Team: Yichen Le yl4347, Yuheng Shen ys2393, Linyi Xia lx277, Jia Liu jl4769, Rongjian Zhai rz495"
format: 
  pdf:
    toc: true
    number-depth: 3
editor: source
number-sections: true
---

## Domain problem formulation

We aim to build a robust workflow that predicts Eastern Bluebird occurrences across their range in the eastern United States and southern Canada. The predictors appear to come from gridded environmental products summarizing land cover composition and topography. There is no accompanying metadata, so part of the project involves confirming how these variables were constructed (e.g., spatial resolution, temporal coverage, buffer size). Understanding the provenance of each variable is essential before interpreting model results or making ecological claims.

## Step 1: Review background information {#sec:bg-info}

### Information on data collection 

The source file is distributed as `EasternBluebird.csv`, a comma-separated export from an unknown system. No formal documentation accompanied the file. Key open questions:

- Who produced the CSV export and at what stage of processing (raw observations, processed grids, etc.)?
- Do the land cover percentages represent a single year, a multi-year average, or multiple concentric buffers stacked together (totals sometimes exceed 100%)?
- Are repeated latitude/longitude pairs distinct surveys through time or different subsamples of the same remote-sensing grid?

### Data dictionary

Column descriptions below are inferred from header names and exploratory analysis. They should be verified against official documentation when it becomes available.

```{r data-dictionary}
#| label: data-dictionary
#| message: false
#| warning: false

tibble::tribble(
  ~column, ~description,
  "LATITUDE", "Latitude of the sampling footprint in decimal degrees (WGS84).",
  "LONGITUDE", "Longitude of the sampling footprint in decimal degrees (WGS84, negative for West).",
  "ELEV", "Elevation of the footprint in meters above sea level (negative values indicate locations below sea level).",
  "Shallow_Ocean", "Percent of the footprint classified as shallow ocean water.",
  "CoastShore_lines", "Percent of the footprint flagged as coastal shoreline interface.",
  "Shallow_Inland", "Percent of the footprint covered by shallow inland water bodies.",
  "Deep_Inland", "Percent of the footprint covered by deep inland water bodies.",
  "Moderate_Ocean", "Percent of the footprint in moderate-depth ocean water.",
  "Deep_Ocean", "Percent of the footprint in deep ocean water.",
  "Evergreen_needle", "Percent evergreen needleleaf forest cover.",
  "Grasslands", "Percent grassland cover.",
  "Croplands", "Percent cropland or agricultural cover.",
  "Urban_Built", "Percent urban or built-up land cover.",
  "Barren", "Percent barren land (bare soil/rock).",
  "Evergreen_broad", "Percent evergreen broadleaf forest cover.",
  "Deciduous_needle", "Percent deciduous needleleaf forest cover.",
  "Deciduous_broad", "Percent deciduous broadleaf forest cover.",
  "Mixed_forest", "Percent mixed forest cover.",
  "Closed_shrubland", "Percent closed shrubland cover.",
  "Open_shrubland", "Percent open shrubland cover.",
  "Woody_savannas", "Percent woody savanna cover.",
  "Savannas", "Percent savanna cover.",
  "y", "Binary indicator of Eastern Bluebird presence (1) or absence (0) for this footprint."
) |>
  knitr::kable()
```

## Step 2: Load the data

We ingest the CSV directly with `readr::read_csv` and standardize column names for downstream wrangling.

```{r setup}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(janitor)
```

```{r data-path}
#| label: data-path
#| message: false
#| warning: false

data_path <- "EasternBluebird.csv"

file_details <- file.info(data_path)
tibble(
  file_size_mb = round(file_details$size / 1024^2, 2),
  last_modified = file_details$mtime
)
```

```{r ingest}
#| label: ingest
#| message: false
#| warning: false

ingest_eastern_bluebird <- function(path) {
  readr::read_csv(path, show_col_types = FALSE)
}

bluebird_raw <- ingest_eastern_bluebird(data_path)
bluebird_raw |> glimpse()
```

```{r initial-clean}
#| label: initial-clean
#| message: false
#| warning: false

landcover_cols <- c(
  "shallow_ocean", "coast_shore_lines", "shallow_inland", "deep_inland",
  "moderate_ocean", "deep_ocean", "evergreen_needle", "grasslands", "croplands",
  "urban_built", "barren", "evergreen_broad", "deciduous_needle",
  "deciduous_broad", "mixed_forest", "closed_shrubland", "open_shrubland",
  "woody_savannas", "savannas"
)

bluebird_ingested <- bluebird_raw |>
  janitor::clean_names() |>
  mutate(
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude),
    landcover_total = rowSums(across(all_of(landcover_cols))),
    landcover_total_over_100 = landcover_total > 100 + 1e-6
  )

tibble(
  rows = nrow(bluebird_ingested),
  columns = ncol(bluebird_ingested)
)
```

```{r preview}
#| label: preview
#| message: false
#| warning: false

bluebird_ingested |>
  slice_head(n = 5)
```

## Step 3: Examine the data

We inspect the loaded tibble to confirm that values sit within expected ranges and to flag any issues that may require cleaning.

### Invalid values

```{r range-summary}
#| label: range-summary
#| message: false
#| warning: false

bluebird_ingested |>
  summarise(
    lat_min = min(latitude),
    lat_max = max(latitude),
    lon_min = min(longitude),
    lon_max = max(longitude),
    elev_min = min(elev),
    elev_max = max(elev)
  )
```

Coordinates fall within the eastern United States and southern Canada, and elevations remain plausible for terrestrial sites.

### Missing values

```{r missing-values}
#| label: missing-values
#| message: false
#| warning: false

bluebird_ingested |>
  summarise(across(everything(), ~ sum(is.na(.)))) |>
  tidyr::pivot_longer(everything(), names_to = "column", values_to = "missing_count") |>
  filter(missing_count > 0)
```

No missing values are present, so downstream analyses can proceed without imputation.

### Data format

```{r duplicates}
#| label: duplicates
#| message: false
#| warning: false

duplicate_summary <- bluebird_ingested |>
  count(latitude, longitude, name = "n_records") |>
  arrange(desc(n_records))

duplicate_summary |>
  summarise(
    total_sites = n(),
    max_records = max(n_records),
    sites_with_duplicates = sum(n_records > 1),
    pct_sites_with_duplicates = scales::percent(sites_with_duplicates / total_sites)
  )
```

Repeated latitude/longitude footprints suggest the data contain multiple surveys per site. Any train/test split should keep replicated footprints together.

### Column names

```{r column-names}
#| label: column-names
#| message: false
#| warning: false

tibble(column = names(bluebird_ingested))
```

`janitor::clean_names()` already produces snake_case headers that are consistent across the dataset.

### Variable type

```{r variable-types}
#| label: variable-types
#| message: false
#| warning: false

tibble(
  column = names(bluebird_ingested),
  class = purrr::map_chr(bluebird_ingested, ~ paste(class(.x), collapse = ", "))
)
```

Predictors are numeric, and the response `y` remains coded as 0/1. If factor semantics are required, the conversion can occur closer to modeling.

### Data specific explorations

```{r landcover-totals}
#| label: landcover-totals
#| message: false
#| warning: false

bluebird_ingested |>
  summarise(
    min_total = min(landcover_total),
    p10_total = quantile(landcover_total, 0.10),
    median_total = median(landcover_total),
    mean_total = mean(landcover_total),
    p90_total = quantile(landcover_total, 0.90),
    max_total = max(landcover_total),
    pct_over_100 = scales::percent(mean(landcover_total_over_100))
  )
```

Land cover totals cluster around 100 but occasionally exceed it, reinforcing the need to confirm whether overlapping buffers or stacked categories generated the export.

## Step 4: Clean the data

The cleaning workflow below:

1. Ingests the CSV and standardizes column names.
2. Computes land cover totals and flags rows where totals exceed 100.
3. Adds normalized land cover fractions so that each record sums to one regardless of the original scale.
4. Aggregates to a site-level table (one row per latitude/longitude) with counts of replicate observations and the mean/any presence outcomes. This reduces leakage when splitting data by location.

```{r cleaning-function}
#| label: cleaning-function
#| message: false
#| warning: false

clean_eastern_bluebird <- function(path) {
  raw <- ingest_eastern_bluebird(path)
  cleaned <- raw |>
    janitor::clean_names() |>
    mutate(
      latitude = as.numeric(latitude),
      longitude = as.numeric(longitude),
      landcover_total = rowSums(across(all_of(landcover_cols))),
      landcover_total_over_100 = landcover_total > 100 + 1e-6
    ) |>
    mutate(
      across(
        all_of(landcover_cols),
        ~ if_else(landcover_total == 0, 0, .x / landcover_total),
        .names = "{.col}_frac"
      )
    )

  site_level <- cleaned |>
    group_by(latitude, longitude) |>
    summarise(
      n_observations = n(),
      presence_any = as.integer(any(y == 1)),
      presence_rate = mean(y),
      elev_mean = mean(elev),
      across(all_of(landcover_cols), mean, .names = "{.col}_mean"),
      across(ends_with("_frac"), mean),
      landcover_total_mean = mean(landcover_total),
      .groups = "drop"
    )

  list(
    raw = raw,
    cleaned = cleaned,
    site_level = site_level
  )
}

bluebird_outputs <- clean_eastern_bluebird(data_path)

bluebird_outputs$site_level |>
  summarise(
    sites = n(),
    mean_records_per_site = mean(n_observations),
    max_records_per_site = max(n_observations)
  )
```

```{r cleaned-preview}
#| label: cleaned-preview
#| message: false
#| warning: false

bluebird_outputs$cleaned |>
  select(
    latitude, longitude, elev, landcover_total,
    landcover_total_over_100, woody_savannas, woody_savannas_frac, y
  ) |>
  slice_head(n = 5)
```

```{r site-preview}
#| label: site-preview
#| message: false
#| warning: false

bluebird_outputs$site_level |>
  slice_head(n = 5)
```

```{r site-target-summary}
#| label: site-target-summary
#| message: false
#| warning: false

bluebird_outputs$site_level |>
  summarise(
    min_rate = min(presence_rate),
    mean_rate = mean(presence_rate),
    median_rate = median(presence_rate),
    max_rate = max(presence_rate)
  )
```

Next steps:

- Confirm the interpretation of land cover totals that exceed 100 and determine whether they require re-normalization or stratified handling.
- Decide on a final export format (CSV, parquet, or RDS) for both the observation-level and site-level tables once metadata questions are resolved.
- Incorporate temporal information if it is available elsewhere so we can respect survey-years during modeling splits.
