---
title: "Data cleaning for Exploratory Data Analysis Eastern Bluebird Capstone"
author: "Team: Yichen Le yl4347, Yuheng Shen ys2393, Linyi Xia lx277, Jia Liu jl4769, Rongjian Zhai rz495"
format: 
  pdf:
    toc: true
    number-depth: 3
editor: source
number-sections: true
---

## Domain problem formulation

We aim to build a robust workflow that predicts Eastern Bluebird occurrences across their range in the eastern United States and southern Canada. The predictors appear to come from gridded environmental products summarizing land cover composition and topography. There is no accompanying metadata, so part of the project involves confirming how these variables were constructed (e.g., spatial resolution, temporal coverage, buffer size). Understanding the provenance of each variable is essential before interpreting model results or making ecological claims.

## Step 1: Review background information {#sec:bg-info}

### Information on data collection 

The source file is distributed as `EasternBluebird.csv`, a comma-separated export from an unknown system. No formal documentation accompanied the file. Key open questions:

- Who produced the CSV export and at what stage of processing (raw observations, processed grids, etc.)?
- Do the land cover percentages represent a single year, a multi-year average, or multiple concentric buffers stacked together (totals sometimes exceed 100%)?
- Are repeated latitude/longitude pairs distinct surveys through time or different subsamples of the same remote-sensing grid?

### Data dictionary

Column descriptions below are inferred from header names and exploratory analysis. They should be verified against official documentation when it becomes available.

```{r}
data_dictionary <- data.frame(
  column = c(
    "LATITUDE", "LONGITUDE", "ELEV", "Shallow_Ocean", "CoastShore_lines",
    "Shallow_Inland", "Deep_Inland", "Moderate_Ocean", "Deep_Ocean",
    "Evergreen_needle", "Grasslands", "Croplands", "Urban_Built", "Barren",
    "Evergreen_broad", "Deciduous_needle", "Deciduous_broad", "Mixed_forest",
    "Closed_shrubland", "Open_shrubland", "Woody_savannas", "Savannas", "y"
  ),
  description = c(
    "Latitude of the sampling footprint in decimal degrees (WGS84).",
    "Longitude of the sampling footprint in decimal degrees (WGS84, negative for West).",
    "Elevation of the footprint in meters above sea level (negative values indicate locations below sea level).",
    "Percent of the footprint classified as shallow ocean water.",
    "Percent of the footprint flagged as coastal shoreline interface.",
    "Percent of the footprint covered by shallow inland water bodies.",
    "Percent of the footprint covered by deep inland water bodies.",
    "Percent of the footprint in moderate-depth ocean water.",
    "Percent of the footprint in deep ocean water.",
    "Percent evergreen needleleaf forest cover.",
    "Percent grassland cover.",
    "Percent cropland or agricultural cover.",
    "Percent urban or built-up land cover.",
    "Percent barren land (bare soil/rock).",
    "Percent evergreen broadleaf forest cover.",
    "Percent deciduous needleleaf forest cover.",
    "Percent deciduous broadleaf forest cover.",
    "Percent mixed forest cover.",
    "Percent closed shrubland cover.",
    "Percent open shrubland cover.",
    "Percent woody savanna cover.",
    "Percent savanna cover.",
    "Binary indicator of Eastern Bluebird presence (1) or absence (0) for this footprint."
  ),
  stringsAsFactors = FALSE
)

knitr::kable(data_dictionary)
```

## Step 2: Load the data

We load the CSV with base R's `read.csv` and convert key columns to numeric so later steps are straightforward.

```{r}
options(stringsAsFactors = FALSE)
```

```{r}
data_path <- "EasternBluebird.csv"
file_details <- file.info(data_path)
file_overview <- data.frame(
  file_size_mb = round(file_details$size / 1024^2, 2),
  last_modified = file_details$mtime
)
file_overview
```

```{r}
bluebird_raw <- read.csv(data_path, stringsAsFactors = FALSE)
str(bluebird_raw)
```

```{r}
bluebird_data <- bluebird_raw
bluebird_data$LATITUDE <- as.numeric(bluebird_data$LATITUDE)
bluebird_data$LONGITUDE <- as.numeric(bluebird_data$LONGITUDE)

landcover_cols <- c(
  "Shallow_Ocean", "CoastShore_lines", "Shallow_Inland", "Deep_Inland",
  "Moderate_Ocean", "Deep_Ocean", "Evergreen_needle", "Grasslands", "Croplands",
  "Urban_Built", "Barren", "Evergreen_broad", "Deciduous_needle",
  "Deciduous_broad", "Mixed_forest", "Closed_shrubland", "Open_shrubland",
  "Woody_savannas", "Savannas"
)

bluebird_data$landcover_total <- rowSums(bluebird_data[, landcover_cols], na.rm = TRUE)
bluebird_data$landcover_total_over_100 <- bluebird_data$landcover_total > (100 + 1e-6)

data.frame(
  rows = nrow(bluebird_data),
  columns = ncol(bluebird_data)
)
```

```{r}
head(bluebird_data, 5)
```

## Step 3: Examine the data

We inspect the loaded data frame to confirm that values sit within expected ranges and to flag any issues that may require cleaning.

### Invalid values

```{r}
range_summary <- data.frame(
  lat_min = min(bluebird_data$LATITUDE, na.rm = TRUE),
  lat_max = max(bluebird_data$LATITUDE, na.rm = TRUE),
  lon_min = min(bluebird_data$LONGITUDE, na.rm = TRUE),
  lon_max = max(bluebird_data$LONGITUDE, na.rm = TRUE),
  elev_min = min(bluebird_data$ELEV, na.rm = TRUE),
  elev_max = max(bluebird_data$ELEV, na.rm = TRUE)
)
range_summary
```

Coordinates fall within the eastern United States and southern Canada, and elevations remain plausible for terrestrial sites.

### Missing values

```{r}
missing_counts <- colSums(is.na(bluebird_data))
missing_counts[missing_counts > 0]
```

No missing values are present, so downstream analyses can proceed without imputation.

### Data format

```{r}
site_keys <- paste(bluebird_data$LATITUDE, bluebird_data$LONGITUDE)
site_table <- table(site_keys)
total_sites <- length(site_table)
max_records <- max(site_table)
sites_with_duplicates <- sum(site_table > 1)
pct_sites_with_duplicates <- round((sites_with_duplicates / total_sites) * 100, 2)
duplicate_summary <- data.frame(
  total_sites = total_sites,
  max_records = max_records,
  sites_with_duplicates = sites_with_duplicates,
  pct_sites_with_duplicates = paste0(pct_sites_with_duplicates, "%")
)
duplicate_summary
```

Repeated latitude/longitude footprints suggest the data contain multiple surveys per site. Any train/test split should keep replicated footprints together.

### Column names

```{r}
data.frame(column = names(bluebird_data))
```

Column names already use underscores, so we keep them as provided.

### Variable type

```{r}
variable_types <- data.frame(
  column = names(bluebird_data),
  class = sapply(bluebird_data, function(x) paste(class(x), collapse = ", "))
)
variable_types
```

Predictors are numeric, and the response `y` remains coded as 0/1. If factor semantics are required, the conversion can occur closer to modeling.

### Data specific explorations

```{r}
landcover_stats <- data.frame(
  min_total = min(bluebird_data$landcover_total, na.rm = TRUE),
  p10_total = as.numeric(quantile(bluebird_data$landcover_total, 0.10, na.rm = TRUE)),
  median_total = median(bluebird_data$landcover_total, na.rm = TRUE),
  mean_total = mean(bluebird_data$landcover_total, na.rm = TRUE),
  p90_total = as.numeric(quantile(bluebird_data$landcover_total, 0.90, na.rm = TRUE)),
  max_total = max(bluebird_data$landcover_total, na.rm = TRUE),
  pct_over_100 = paste0(round(mean(bluebird_data$landcover_total_over_100, na.rm = TRUE) * 100, 2), "%")
)
landcover_stats
```

Land cover totals cluster around 100 but occasionally exceed it, reinforcing the need to confirm whether overlapping buffers or stacked categories generated the export.

## Step 4: Clean the data

The cleaning workflow below:

1. Copies the raw table and stores latitude/longitude as numeric values.
2. Computes land cover totals and flags rows where totals exceed 100.
3. Adds normalized land cover fractions so each record sums to one even if the original totals differ.
4. Aggregates to a site-level table (one row per latitude/longitude) with counts of replicate observations and simple presence summaries. This reduces leakage when splitting data by location.

```{r}
bluebird_simple <- bluebird_data

for (col in landcover_cols) {
  new_name <- paste0(col, "_frac")
  bluebird_simple[[new_name]] <- ifelse(
    bluebird_simple$landcover_total == 0,
    0,
    bluebird_simple[[col]] / bluebird_simple$landcover_total
  )
}

site_counts <- aggregate(y ~ LATITUDE + LONGITUDE, data = bluebird_simple, length)
names(site_counts)[3] <- "n_observations"

presence_any <- aggregate(y ~ LATITUDE + LONGITUDE, data = bluebird_simple, function(x) as.integer(any(x == 1)))
names(presence_any)[3] <- "presence_any"

presence_rate <- aggregate(y ~ LATITUDE + LONGITUDE, data = bluebird_simple, mean)
names(presence_rate)[3] <- "presence_rate"

elev_mean <- aggregate(ELEV ~ LATITUDE + LONGITUDE, data = bluebird_simple, mean)
names(elev_mean)[3] <- "elev_mean"

landcover_total_mean <- aggregate(landcover_total ~ LATITUDE + LONGITUDE, data = bluebird_simple, mean)
names(landcover_total_mean)[3] <- "landcover_total_mean"

site_level <- merge(site_counts, presence_any, by = c("LATITUDE", "LONGITUDE"))
site_level <- merge(site_level, presence_rate, by = c("LATITUDE", "LONGITUDE"))
site_level <- merge(site_level, elev_mean, by = c("LATITUDE", "LONGITUDE"))
site_level <- merge(site_level, landcover_total_mean, by = c("LATITUDE", "LONGITUDE"))

bluebird_clean_list <- list(
  cleaned = bluebird_simple,
  site_level = site_level
)

site_overview <- data.frame(
  sites = nrow(site_level),
  mean_records_per_site = mean(site_level$n_observations),
  max_records_per_site = max(site_level$n_observations)
)
site_overview
```

```{r}
columns_to_show <- c(
  "LATITUDE", "LONGITUDE", "ELEV", "landcover_total",
  "landcover_total_over_100", "Woody_savannas", "Woody_savannas_frac", "y"
)
head(bluebird_clean_list$cleaned[, columns_to_show], 5)
```

```{r}
head(bluebird_clean_list$site_level, 5)
```

```{r}
site_stats <- data.frame(
  min_rate = min(bluebird_clean_list$site_level$presence_rate, na.rm = TRUE),
  mean_rate = mean(bluebird_clean_list$site_level$presence_rate, na.rm = TRUE),
  median_rate = median(bluebird_clean_list$site_level$presence_rate, na.rm = TRUE),
  max_rate = max(bluebird_clean_list$site_level$presence_rate, na.rm = TRUE)
)
site_stats
```

Next steps:

- Confirm the interpretation of land cover totals that exceed 100 and determine whether they require re-normalization or stratified handling.
- Decide on a final export format (CSV, parquet, or RDS) for both the observation-level and site-level tables once metadata questions are resolved.
- Incorporate temporal information if it is available elsewhere so we can respect survey-years during modeling splits.